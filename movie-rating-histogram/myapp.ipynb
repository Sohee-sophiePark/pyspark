{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Detecting Spark in Conda Environment\r\n",
    "\r\n",
    "Detecting spark inside the conda environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# pip install findspark \r\n",
    "\r\n",
    "import findspark # must the very first line\r\n",
    "findspark.init()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Set Up the Entry Point for Spark Environment\r\n",
    "\r\n",
    "- SparkConf: Configuration Parameters\r\n",
    "- SparkContext: Role as a SparkSession"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from pyspark import SparkConf, SparkContext # mostly initialized in python script\r\n",
    "import collections # sorting / ordering"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Set Up the Context for the Node and Session"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RatingHistogram\")\r\n",
    "sc = SparkContext(conf = conf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Load Data into RDD"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# set the path and load data from it\r\n",
    "path = \".\\\\data-100k\\\\u.data\"\r\n",
    "\r\n",
    "# breaks file into line by line for each corresponding to value in RDD\r\n",
    "# create the first RDD named 'lines'\r\n",
    "lines = sc.textFile(path) # RDD"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Extract Data "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# extract (map) data - userID | moiveID | ratings | timestamps with index\r\n",
    "# mapping every sigle row with the function split() and take the third column value\r\n",
    "\r\n",
    "# create the second RDD named 'ratings'\r\n",
    "ratings = lines.map(lambda x: x.split()[2]) \r\n",
    "print(ratings)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PythonRDD[7] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Aggregation - PERFORM AN ACTION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Assign new RDD from transformed the prev RDD\r\n",
    "\r\n",
    "# create the third RDD named 'result'\r\n",
    "result = ratings.countByValue() # count every occurences for the values in the ratings\r\n",
    "print(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "defaultdict(<class 'int'>, {'3': 27145, '1': 6110, '2': 11370, '4': 34174, '5': 21201})\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Sorting (Ordering by Value)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "sortedResults = collections.OrderedDict(sorted(result.items()))\r\n",
    "for key, value in sortedResults.items():\r\n",
    "    print(\"%s %i\" % (key, value))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 6110\n",
      "2 11370\n",
      "3 27145\n",
      "4 34174\n",
      "5 21201\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('env-pyspark': conda)"
  },
  "interpreter": {
   "hash": "6150b5af2f35b99a2bcf5a34155fe572fa7529cb6a8cda12ad6116215b21597b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}